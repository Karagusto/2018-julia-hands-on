{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><span style=\"color:red\">Distributed Computing with Julia</span></center>\n",
    "\n",
    "---\n",
    "\n",
    "# Overview of Distributed Programming\n",
    "\n",
    "### Sequential execution\n",
    "\n",
    "* Our minds are single-core\n",
    " * pseudo-parallelism tricks us\n",
    " * e.g., round-robin scheduling algorithm\n",
    "\n",
    "### Memory\n",
    "\n",
    "* we only know what is stored at our memory (at least we hope so)\n",
    "* we do not precisely know what is inside of others' memory\n",
    "\n",
    "### Cooperative tasks $\\rightarrow$ Communication\n",
    "\n",
    "* suppose that we need to do a cooperative work together\n",
    "* E.g.: writing a paper\n",
    "* for that, we need to share data $\\rightarrow$ we need to communicate\n",
    "\n",
    "### Distributed communication\n",
    "\n",
    "* **message passing**\n",
    " * the most basic communication primitive of computer systems\n",
    " * mimcs how human beings have been communicating to each other\n",
    " * hard to manage the data since they are spread among processors\n",
    " * Approaches: MPI, RPC, RMI, HTTP, etc.\n",
    "* **shared memory**\n",
    " * requires a higher-level programming support (indirect communication)\n",
    " * easier to communicate (to code)\n",
    " * might creates bottlnecks (the shared memory)\n",
    " * Indirect Communication: when we need loosely coupled (time and space)\n",
    "      * shared memory (Distributed File System like HDFS, etc.)\n",
    "      * group communication\n",
    "      * pub-subscribe systems (event-based)\n",
    "      * message queues\n",
    "      * produce-consume\n",
    "      * ...\n",
    "  \n",
    "### Example\n",
    "\n",
    "* Message passing\n",
    "\n",
    "E.g. me $\\rightarrow$ audience = message passing (broadcast, multicast, point-to-point)\n",
    "\n",
    "* Shared memory\n",
    "\n",
    "E.g. board (presentation) $\\rightarrow$ audience = shared memory, e.g., $f(x)=2x$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master/Worker programming pattern\n",
    "\n",
    "<img src=\"figures/master-worker.svg\" width=\"425\" height=\"300\" alt=\"Master/Worker\"></a>\n",
    "\n",
    "\n",
    "Julia relies on the Master/Worker paradigm. While the master process controls the distributed computation, the **workers performs all the computation**.\n",
    "\n",
    "\n",
    "* Each process has an associated identifier\n",
    "    * **Master** process ID (PID) = 1\n",
    "    * **Workers** process IDs (PID) >= 2 \n",
    "* Only workers execute distributed calls\n",
    "    * Except in a single-process environment\n",
    "        * Process 1 is both master and worker\n",
    "        * I.e., Julia interactive prompt\n",
    "\n",
    "## Managing distributed processes\n",
    "\n",
    "### Getting the current processes (master and workers)\n",
    "\n",
    "* [`nprocs()`](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.Distributed.nprocs)\n",
    "* [`procs()`](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.Distributed.procs-Tuple{SharedArray})\n",
    "* [`nworkers()`](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.Distributed.nworkers)\n",
    "* [`workers()`](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.Distributed.workers)\n",
    "\n",
    "### Adding workers\n",
    "\n",
    "* [`addprocs(...)`](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.Distributed.addprocs):\n",
    "\n",
    "### Removing workers\n",
    "\n",
    "* \n",
    "\n",
    "Let's use [`nprocs()`](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.Distributed.nprocs) to know the number of process in use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "procs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either add more processes via **SSH** or add more **local processors** to take advantage of multi-core computers with [addprocs(...)](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.Distributed.addprocs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Int64,1}:\n",
       " 2\n",
       " 3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addprocs(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then list again to see the just-added processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nprocs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the list of processes with [procs()](http://docs.julialang.org/en/release-0.3/stdlib/parallel/#Base.procs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "procs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, you can list only the [workers()](http://docs.julialang.org/en/release-0.3/stdlib/parallel/#Base.workers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Int64,1}:\n",
       " 3\n",
       " 4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also iterate over the `workers()` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing worker #3\n",
      "Listing worker #4\n"
     ]
    }
   ],
   "source": [
    "for pid in workers()\n",
    "    println(\"Listing worker #\", pid)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will remove both workers (the processes we previously added) with [`rmprocs(pids...)`](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.Distributed.rmprocs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Task (done) @0x000000011a677850"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmprocs(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Int64,1}:\n",
       " 1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "procs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Exercise</span>\n",
    "\n",
    "\n",
    "<span style=\"color:green\">Add 2 workers and then remove them assuming that you don't know their IDs.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Clusters](http://docs.julialang.org/en/release-0.3/manual/parallel-computing/?highlight=cluster#clustermanagers)\n",
    "\n",
    "\n",
    "## Basics\n",
    "\n",
    "* Julia processes run on Julia [Clusters](http://docs.julialang.org/en/release-0.3/manual/parallel-computing/?highlight=cluster#clustermanagers)\n",
    "    * **Local** cluster\n",
    "        * Leverage local multi-cores \n",
    "    * **Distributed** cluster\n",
    "        * On top of SSH/TCP/IP connections\n",
    "* Clusters can be customized\n",
    "\n",
    "* Clusters can be created as Julia starts. For instance, **to create a 2-core local cluster**:\n",
    "\n",
    "```\n",
    "$ julia -p 2\n",
    "```\n",
    "\n",
    "* A **distributed SSH cluster** can be created as follows:\n",
    "\n",
    "```\n",
    "julia --machinefile HOSTS_FILE common_code.jl\n",
    "```\n",
    "\n",
    "* Julia will start worker processes remotely through a password-less SSH login on the computers listed at the HOSTS_FILE file. In addition, it will deploy the *common_code.jl* file on all computers.\n",
    "\n",
    "* It is important to remark that these computers can be any computer reachable by password-less SSH logins. For example, LAN computers, Amazon EC2 instances, [Docker](http://docker.com) containers, and do forth.\n",
    "\n",
    "## Advanced\n",
    "\n",
    "* See [Cluster Manager Interface](https://docs.julialang.org/en/stable/stdlib/parallel/#Cluster-Manager-Interface-1)\n",
    "    \n",
    "\n",
    "# Cloud Computing support\n",
    "\n",
    "### [JuliaRun](https://juliacomputing.com/products/juliarun.html)\n",
    "\n",
    "* Propriertary and payed solution from [Julia Computing](https://juliacomputing.com)\n",
    "* JuliaRun provides batch and interactive deployment in the Cloud\n",
    "\n",
    "\n",
    "### [Infra.jl]()\n",
    "\n",
    "* **An automatic deployment support for processing remote sensing data in the Cloud**\n",
    "* Paper presented at [IGARSS 2018 - Invited Special Session: Earth Data for Global Scale Applications](https://www.igarss2018.org/Papers/PublicSessionIndex3.asp?Sessionid=1310)\n",
    " * Actually, was presented today :-) Tuesday, July 24, 09:10 - 09:30 (Valencia, Spain timezone)\n",
    "\n",
    "\n",
    "\n",
    "<!--\n",
    "# Cloud support\n",
    "\n",
    "## Virtual machines (VMs)\n",
    "\n",
    "* [`Amazon Web Services (AWS)`](https://github.com/amitmurthy/AWS.jl)\n",
    "* [`OpenStack.jl`](https://github.com/Keno/OpenStack.jl/blob/master/src/OpenStack.jl)\n",
    "    * Uses [OpenStack Restful API](http://developer.openstack.org/api-ref-compute-v2.html)\n",
    "\n",
    "## Containers\n",
    "\n",
    "* [Infra.jl](https://github.com/gsd-ufal/CloudArray.jl/blob/master/src/Infra.jl)\n",
    "    * Set of **management functions for managing Julia workers on Docker**\n",
    "* [Docker.jl](https://github.com/Keno/Docker.jl)\n",
    "    * Wrapper to [Docker HTTP API](https://docs.docker.com/reference/api/docker_remote_api/)\n",
    "\n",
    "## Higher-level support \n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [<span style=\"color:red\">Julia programming support for distributed communication</span>](https://docs.julialang.org/en/latest/manual/parallel-computing/)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Let's read the first paragraphs of the [Julia Official Documentation on Parallel Computing](https://docs.julialang.org/en/latest/manual/parallel-computing/)\n",
    "\n",
    "* \"Julia’s implementation of message passing is **different from other environments such as MPI**. \n",
    "* **Communication in Julia is generally “one-sided”**\n",
    "    * The programmer needs **to explicitly manage only one process in a two-process operation**. \n",
    "    * These operations typically **do not look like “message send” and “message receive”** but rather resemble **higher-level operations like calls to user functions**.\n",
    "\n",
    "* Parallel programming in Julia is built on two primitives: **remote references** and **remote calls**. \n",
    "    * A **remote reference** is an object that can be used from any process to **refer to an object stored on a particular process.**\n",
    "    * A **remote call** is a **request by one process to call a certain function** on certain arguments on another (possibly the same) process. \n",
    "        * A remote call **returns a [`Future`](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.Distributed.Future)** to its result. \n",
    "        * Remote calls return immediately (**Asynchronous**); the process that made the call proceeds to its next operation while the remote call happens somewhere else. \n",
    "        * You can wait for a remote call to finish by calling [`wait`](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.wait) on the returned Future\n",
    "        * You can obtain the full value of the result using [`fetch()`](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.fetch-Tuple{Channel}).\n",
    "\n",
    "* [...] multiple processes can co-ordinate their processing by referencing the same remote Channel.on its remote reference (**Synchronous**).\n",
    " * You can obtain the full value of the result using **fetch**. \n",
    " * You can store a value to a remote reference using **put**.\"\n",
    "\n",
    "#### In other words...\n",
    "\n",
    "* It is **not a send-receive paradigm**\n",
    "    * <span style=\"color:red\">There is no receive: it assumes that communication is managed by the calling process</span>\n",
    "    * <span style=\"color:red\">The calling process calls remote functions or expressions</span>\n",
    "    \n",
    "<img src=\"figures/master-worker.svg\" width=\"425\" height=\"300\" alt=\"Master/Worker\"></a>\n",
    " \n",
    "#### Further supports\n",
    "\n",
    "* [**Macros**](https://docs.julialang.org/en/stable/manual/parallel-computing/) to make communication easier\n",
    "* **Tasks**\n",
    "    * Coroutines, green threads that run on a single actual thread\n",
    "    * **Tasks can be combined with remote calls for parallel programming**\n",
    "    * [Julia multithreading](https://docs.julialang.org/en/latest/base/multi-threading/): on-going work (experimental)   \n",
    "    \n",
    "## Programming model\n",
    "\n",
    "\n",
    "* Programming model\n",
    "    * **Primitive: remote calls**\n",
    "        * A remote request similiar to _send_\n",
    "            * However, remote calls specify the operation to be performed\n",
    "            * Similar to embed the _receive_ operation into _send_\n",
    "        * [`remotecall(id, func, args...)`](http://docs.julialang.org/en/release-0.3/stdlib/parallel/#Base.remotecall)\n",
    "            * `id` is the process that will perform the computation\n",
    "            * `func` is the function or expression to be executed\n",
    "            * `args` ar the function arguments\n",
    "    * **Abstraction: remote reference**\n",
    "        * Remote calls return a remote reference (`RemoteRef`)\n",
    "        * `RemoteRef` can be used by any processes\n",
    "    \n",
    "## Julia support for distributed communication - <span style=\"color:red\">In a nutshell</span>\n",
    "\n",
    "### 1. Add Workers\n",
    "\n",
    "In order to put workers to execute some code, let's add two processes since we currently have a single (master) process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Int64,1}:\n",
       " 8\n",
       " 9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addprocs(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Int64,1}:\n",
       " 3\n",
       " 4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. If necessary, let them know about your code\n",
    "\n",
    "* Use [@everywhere](https://docs.julialang.org/en/latest/stdlib/Distributed/#Distributed.@everywhere) macro\n",
    "* See more on [Code Availability and Loading Packages](https://docs.julialang.org/en/latest/manual/parallel-computing/#Code-Availability-and-Loading-Packages-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere my_function(x,y) = x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Ask Workers to process what you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Future(8, 1, 18, Nullable{Any}())"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remotecall(my_function,8,10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Future(8, 1, 19, Nullable{Any}())"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_reference = remotecall(my_function,8,10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future(2, 1, 15, Nullable{Any}())\n"
     ]
    }
   ],
   "source": [
    "println(output_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Get the result back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch(output_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark\n",
    "\n",
    "We can perform Steps 3 and 4 at once in an elegant fasion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@fetchfrom 3 my_function(17,24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we don't need to specify the Worker that will process the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@fetch my_function(120,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# A more comprenhesive overview of distributed communication in Julia\n",
    "\n",
    "## [Remote calls](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.Distributed.remotecall-Tuple{Any,Integer,Vararg{Any,N}%20where%20N}) and Synchronism\n",
    "\n",
    "* Julia low-level communication primitives\n",
    "* Remote calls can be performed **Synchronously** or **Asynchronously**.\n",
    "\n",
    "\n",
    "### Asynchronous remote calls\n",
    "\n",
    "* The calling process **does not wait** the worker computation\n",
    "    * **Non-blocking call**\n",
    "* [`remotecall(func, worker, args...)`](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.Distributed.remotecall-Tuple{Any,Integer,Vararg{Any,N}%20where%20N})\n",
    "\n",
    "### Synchronous remote calls\n",
    "\n",
    "* **Synchronous** communications block the sender till they receive the response\n",
    "    * Makes the caller **wait**\n",
    "* [`remotecall_wait(func, worker, args...)`](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.Distributed.remotecall_wait-Tuple{Any,Integer,Vararg{Any,N}%20where%20N})\n",
    "\n",
    "The following example puts _Worker 2_ to _sleep 5 seconds_ and waits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This message waited for the Worker\n"
     ]
    }
   ],
   "source": [
    "remotecall_wait(sleep, 6, 5)\n",
    "\n",
    "println(\"This message waited for the Worker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This call blocked the caller process (master, process whose ID is 1) for 5 seconds. \n",
    "\n",
    "Remark that the result from the Output appeared 5 seconds later.\n",
    "\n",
    "Now let's execute the same command **asynchronously** (non-blocking call):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This message did NOT wait for the Worker\n"
     ]
    }
   ],
   "source": [
    "remotecall(sleep, 2, 5)\n",
    "\n",
    "println(\"This message did NOT wait for the Worker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### @sync and @async\n",
    "\n",
    "* Macros that ease the construction of synchronous and asynchronous calls.\n",
    "* These macros return [`Tasks`](https://docs.julialang.org/en/stable/stdlib/parallel/#Tasks-1).\n",
    "* [**`@sync`**](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.@sync)\n",
    "    * Makes the calling process **wait** for a function/expression execution\n",
    "        * _Wait until all dynamically-enclosed uses of @async, @spawn, @spawnat and @parallel are complete._\n",
    "    * Similiar to [monitors](https://en.wikipedia.org/wiki/Monitor_%28synchronization%29)\n",
    "    * Optionally, we can use  `wait` instead:\n",
    "        * [`wait([x])`](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.wait)\n",
    "            * _Block the current task until some event occurs, depending on the type of the argument._\n",
    "        * [`timedwait(testcb, secs; pollint)`](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.timedwait)\n",
    "            * _Waits till `testcb` returns true or for `secs` seconds, whichever is earlier._\n",
    "            * _`testcb` is polled every `pollint` seconds._\n",
    "            \n",
    "* [**`@async`**](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.@async)\n",
    "    * Free the calling process (Task) from a blocked operation\n",
    "    * Similar to running a command in background in Bash\n",
    "   \n",
    "\n",
    "### Remark on Asynchronous vs. Synchronous calls\n",
    "\n",
    "* Usage depends on the distributed application\n",
    "* Use **Synchronous** calls when you need to wait a result to carry on the computation:\n",
    "    * Workflows\n",
    "    * Mutual exclusion: consistency, integrity\n",
    "* Otherwise, use **Asynchronous** calls since blocking communications degrade the system performance and can eventually imply [deadlocks](https://en.wikipedia.org/wiki/Deadlock)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling distributed functions...\n",
      "elapsed time: 3.025307282 seconds\n",
      "This line waited 3.025307282 secs\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "println(\"Calling distributed functions...\")\n",
    "\n",
    "# this @sync blocks the code execution till the end of the for\n",
    "@sync for w in workers() \n",
    "    # this @async allows to launch all workers at once so their execution will be performed in parallel\n",
    "     @async remotecall_wait(sleep, w, 3)\n",
    "end\n",
    "\n",
    "println(\"This line waited $(toc()) secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Exercise</span>\n",
    "\n",
    "\n",
    "<span style=\"color:green\">Remove the `@async` macro from the code above and run it again. Did the code change its behavior? Why? Use the following figure to help your explanation.</span>\n",
    "\n",
    "<img src=\"figures/master-worker.svg\" width=\"300\" alt=\"Master/Worker\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @spawn and @spawnat\n",
    "\n",
    "* The macro [@spawn](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.Distributed.@spawn) spawns (runs) an expression/function at a worker.\n",
    "* All function computation is done at **Worker side**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Future(4, 1, 81, Nullable{Any}())"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_matrix = @spawn rand(2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fetch the `remote_matrix` variable with fetch(...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       " 0.16161   0.341916\n",
       " 0.490374  0.11404 "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch(remote_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to execute an expression over the `remote_matrix`, you can still use @spawn combined with fetch(...):\n",
    "\n",
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Future(2, 1, 33, Nullable{Any}())"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_matrix_plus_one = @spawn 1 .+ fetch(remote_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fetch the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       " 1.41991  1.28423\n",
       " 1.21198  1.15855"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch(remote_matrix_plus_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious about where is `remote_matrix_plus_one`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_matrix_plus_one.where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to have more control of the distributed computation, you may use the macro [@spawnat](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.Distributed.@spawnat) which allows to spawn an expression/function at a specific worker.\n",
    "\n",
    "In the following example, we execute the sqtr(...) function at the worker whose ID is 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Future(3, 1, 83, Nullable{Any}())"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raiz = @spawnat 3 sqrt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[1ma\u001b[22m\u001b[1md\u001b[22m\u001b[1md\u001b[22m\u001b[1mp\u001b[22m\u001b[1mr\u001b[22m\u001b[1mo\u001b[22m\u001b[1mc\u001b[22m\u001b[1ms\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "addprocs(manager::ClusterManager; kwargs...) -> List of process identifiers\n",
       "```\n",
       "\n",
       "Launches worker processes via the specified cluster manager.\n",
       "\n",
       "For example, Beowulf clusters are supported via a custom cluster manager implemented in the package `ClusterManagers.jl`.\n",
       "\n",
       "The number of seconds a newly launched worker waits for connection establishment from the master can be specified via variable `JULIA_WORKER_TIMEOUT` in the worker process's environment. Relevant only when using TCP/IP as transport.\n",
       "\n",
       "```\n",
       "addprocs(machines; tunnel=false, sshflags=``, max_parallel=10, kwargs...) -> List of process identifiers\n",
       "```\n",
       "\n",
       "Add processes on remote machines via SSH. Requires `julia` to be installed in the same location on each node, or to be available via a shared file system.\n",
       "\n",
       "`machines` is a vector of machine specifications. Workers are started for each specification.\n",
       "\n",
       "A machine specification is either a string `machine_spec` or a tuple - `(machine_spec, count)`.\n",
       "\n",
       "`machine_spec` is a string of the form `[user@]host[:port] [bind_addr[:port]]`. `user` defaults to current user, `port` to the standard ssh port. If `[bind_addr[:port]]` is specified, other workers will connect to this worker at the specified `bind_addr` and `port`.\n",
       "\n",
       "`count` is the number of workers to be launched on the specified host. If specified as `:auto` it will launch as many workers as the number of cores on the specific host.\n",
       "\n",
       "Keyword arguments:\n",
       "\n",
       "  * `tunnel`: if `true` then SSH tunneling will be used to connect to the worker from the master process. Default is `false`.\n",
       "  * `sshflags`: specifies additional ssh options, e.g. ```sshflags=`-i /home/foo/bar.pem````\n",
       "  * `max_parallel`: specifies the maximum number of workers connected to in parallel at a host. Defaults to 10.\n",
       "  * `dir`: specifies the working directory on the workers. Defaults to the host's current directory (as found by `pwd()`)\n",
       "  * `enable_threaded_blas`: if `true` then  BLAS will run on multiple threads in added processes. Default is `false`.\n",
       "  * `exename`: name of the `julia` executable. Defaults to `\"$JULIA_HOME/julia\"` or `\"$JULIA_HOME/julia-debug\"` as the case may be.\n",
       "  * `exeflags`: additional flags passed to the worker processes.\n",
       "  * `topology`: Specifies how the workers connect to each other. Sending a message between unconnected workers results in an error.\n",
       "\n",
       "      * `topology=:all_to_all`: All processes are connected to each other. The default.\n",
       "      * `topology=:master_slave`: Only the driver process, i.e. `pid` 1 connects to the workers. The workers do not connect to each other.\n",
       "      * `topology=:custom`: The `launch` method of the cluster manager specifies the connection topology via fields `ident` and `connect_idents` in `WorkerConfig`. A worker with a cluster manager identity `ident` will connect to all workers specified in `connect_idents`.\n",
       "\n",
       "Environment variables :\n",
       "\n",
       "If the master process fails to establish a connection with a newly launched worker within 60.0 seconds, the worker treats it as a fatal situation and terminates. This timeout can be controlled via environment variable `JULIA_WORKER_TIMEOUT`. The value of `JULIA_WORKER_TIMEOUT` on the master process specifies the number of seconds a newly launched worker waits for connection establishment.\n",
       "\n",
       "```\n",
       "addprocs(; kwargs...) -> List of process identifiers\n",
       "```\n",
       "\n",
       "Equivalent to `addprocs(Sys.CPU_CORES; kwargs...)`\n",
       "\n",
       "Note that workers do not run a `.juliarc.jl` startup script, nor do they synchronize their global state (such as global variables, new method definitions, and loaded modules) with any of the other running processes.\n",
       "\n",
       "```\n",
       "addprocs(np::Integer; restrict=true, kwargs...) -> List of process identifiers\n",
       "```\n",
       "\n",
       "Launches workers using the in-built `LocalManager` which only launches workers on the local host. This can be used to take advantage of multiple cores. `addprocs(4)` will add 4 processes on the local machine. If `restrict` is `true`, binding is restricted to `127.0.0.1`. Keyword args `dir`, `exename`, `exeflags`, `topology`, and `enable_threaded_blas` have the same effect as documented for `addprocs(machines)`.\n"
      ],
      "text/plain": [
       "```\n",
       "addprocs(manager::ClusterManager; kwargs...) -> List of process identifiers\n",
       "```\n",
       "\n",
       "Launches worker processes via the specified cluster manager.\n",
       "\n",
       "For example, Beowulf clusters are supported via a custom cluster manager implemented in the package `ClusterManagers.jl`.\n",
       "\n",
       "The number of seconds a newly launched worker waits for connection establishment from the master can be specified via variable `JULIA_WORKER_TIMEOUT` in the worker process's environment. Relevant only when using TCP/IP as transport.\n",
       "\n",
       "```\n",
       "addprocs(machines; tunnel=false, sshflags=``, max_parallel=10, kwargs...) -> List of process identifiers\n",
       "```\n",
       "\n",
       "Add processes on remote machines via SSH. Requires `julia` to be installed in the same location on each node, or to be available via a shared file system.\n",
       "\n",
       "`machines` is a vector of machine specifications. Workers are started for each specification.\n",
       "\n",
       "A machine specification is either a string `machine_spec` or a tuple - `(machine_spec, count)`.\n",
       "\n",
       "`machine_spec` is a string of the form `[user@]host[:port] [bind_addr[:port]]`. `user` defaults to current user, `port` to the standard ssh port. If `[bind_addr[:port]]` is specified, other workers will connect to this worker at the specified `bind_addr` and `port`.\n",
       "\n",
       "`count` is the number of workers to be launched on the specified host. If specified as `:auto` it will launch as many workers as the number of cores on the specific host.\n",
       "\n",
       "Keyword arguments:\n",
       "\n",
       "  * `tunnel`: if `true` then SSH tunneling will be used to connect to the worker from the master process. Default is `false`.\n",
       "  * `sshflags`: specifies additional ssh options, e.g. ```sshflags=`-i /home/foo/bar.pem````\n",
       "  * `max_parallel`: specifies the maximum number of workers connected to in parallel at a host. Defaults to 10.\n",
       "  * `dir`: specifies the working directory on the workers. Defaults to the host's current directory (as found by `pwd()`)\n",
       "  * `enable_threaded_blas`: if `true` then  BLAS will run on multiple threads in added processes. Default is `false`.\n",
       "  * `exename`: name of the `julia` executable. Defaults to `\"$JULIA_HOME/julia\"` or `\"$JULIA_HOME/julia-debug\"` as the case may be.\n",
       "  * `exeflags`: additional flags passed to the worker processes.\n",
       "  * `topology`: Specifies how the workers connect to each other. Sending a message between unconnected workers results in an error.\n",
       "\n",
       "      * `topology=:all_to_all`: All processes are connected to each other. The default.\n",
       "      * `topology=:master_slave`: Only the driver process, i.e. `pid` 1 connects to the workers. The workers do not connect to each other.\n",
       "      * `topology=:custom`: The `launch` method of the cluster manager specifies the connection topology via fields `ident` and `connect_idents` in `WorkerConfig`. A worker with a cluster manager identity `ident` will connect to all workers specified in `connect_idents`.\n",
       "\n",
       "Environment variables :\n",
       "\n",
       "If the master process fails to establish a connection with a newly launched worker within 60.0 seconds, the worker treats it as a fatal situation and terminates. This timeout can be controlled via environment variable `JULIA_WORKER_TIMEOUT`. The value of `JULIA_WORKER_TIMEOUT` on the master process specifies the number of seconds a newly launched worker waits for connection establishment.\n",
       "\n",
       "```\n",
       "addprocs(; kwargs...) -> List of process identifiers\n",
       "```\n",
       "\n",
       "Equivalent to `addprocs(Sys.CPU_CORES; kwargs...)`\n",
       "\n",
       "Note that workers do not run a `.juliarc.jl` startup script, nor do they synchronize their global state (such as global variables, new method definitions, and loaded modules) with any of the other running processes.\n",
       "\n",
       "```\n",
       "addprocs(np::Integer; restrict=true, kwargs...) -> List of process identifiers\n",
       "```\n",
       "\n",
       "Launches workers using the in-built `LocalManager` which only launches workers on the local host. This can be used to take advantage of multiple cores. `addprocs(4)` will add 4 processes on the local machine. If `restrict` is `true`, binding is restricted to `127.0.0.1`. Keyword args `dir`, `exename`, `exeflags`, `topology`, and `enable_threaded_blas` have the same effect as documented for `addprocs(machines)`.\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?addprocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fetch the `raiz` variable with `fetch` and get the worker where `raiz` is stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetch(raiz) = 1.4142135623730951\n",
      "raiz.where = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@show fetch(raiz)\n",
    "\n",
    "@show raiz.where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [@fetch](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.fetch-Tuple{Any}) and [@fetchfrom](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.Distributed.@fetchfrom)\n",
    "\n",
    "We can also use spawn and fetch at once with macro `@fetch(...)` which is equivalent to **`fetch(@spawn expr)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@fetch 2 + 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more control, use the macro `@fetchfrom(...)` which is equivalent to **`fetch(@spawnat p expr)`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5159780352"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@fetchfrom 3 12^9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [@everywhere](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.Distributed.@everywhere)\n",
    "\n",
    "* The [@everywhere](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.Distributed.@everywhere) macro forces a command/expression to run on all processes.\n",
    "* Example: let's print everywhere (at every workers) processes' IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\tFrom worker 2:\t2\n",
      "\tFrom worker 3:\t3\n"
     ]
    }
   ],
   "source": [
    "#addprocs(2)\n",
    "@everywhere println(myid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, let's define on all processes a function that prints workers' IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere who_are_you() = println(\"I'm worker $(myid())\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 3:\tI'm worker 3\n",
      "\tFrom worker 3:\tI'm worker 3\n",
      "\tFrom worker 2:\tI'm worker 2\n",
      "\tFrom worker 2:\tI'm worker 2\n",
      "\tFrom worker 2:\tI'm worker 2\n"
     ]
    }
   ],
   "source": [
    "for i=1:5\n",
    "    @spawn who_are_you()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* @everywhere can also be used to load modules on all processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pkg.add(\"Calculus\")\n",
    "@everywhere using Calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.000000000099304"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@fetch derivative(x-> x^3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For loading [Julia Modules](https://docs.julialang.org/en/stable/manual/modules/) use [`require`(...)](https://docs.julialang.org/en/stable/stdlib/base/#Base.require)\n",
    "* For loading code source files, use [`include(\"file_path\")`](https://docs.julialang.org/en/stable/stdlib/base/#Base.include)\n",
    "\n",
    "Let's make the Workers know about the `MyCode.jl` file and then call its function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere include(\"MyCode.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@fetch custom_function(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function custom_function(α,β,γ)\n",
      "    return α * β - γ\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "print(readstring(\"MyCode.jl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [@parallel](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.Distributed.@parallel)\n",
    "\n",
    "* Parallelizes a for loop or a comprenhesion automatically among the available workers.\n",
    "    * Split the range\n",
    "    * Distribute the range to each process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Future,1}:\n",
       " Future(3, 1, 127, #NULL)\n",
       " Future(2, 1, 128, #NULL)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 3:\t103\n",
      "\tFrom worker 3:\t103\n",
      "\tFrom worker 3:\t103\n",
      "\tFrom worker 3:\t103\n",
      "\tFrom worker 3:\t103\n",
      "\tFrom worker 2:\t102\n",
      "\tFrom worker 2:\t102\n",
      "\tFrom worker 2:\t102\n",
      "\tFrom worker 2:\t102\n",
      "\tFrom worker 2:\t102\n"
     ]
    }
   ],
   "source": [
    "@everywhere plus_my_id(x) = x + myid()\n",
    "\n",
    "a = 100\n",
    "@parallel for i=1:10\n",
    "    println(plus_my_id(a))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`@parallel`** may also performs a **reduce function on each worker and the final reduction at the calling process** (e.g., Master):\n",
    "    * Optionally, @parallel may take a “reducer” as its first argument\n",
    "    * The results from each remote reducer will be aggregated using the reducer locally\n",
    "    * Tasks are assigned asynchronously when no reducer operation is used\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1025000000"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@parallel (+) for i=1:10^7\n",
    "   plus_my_id(a)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# [<span style=\"color:red\">Tasks (coroutines)</span>](https://docs.julialang.org/en/stable/manual/control-flow/#man-tasks-1)\n",
    "\n",
    "\n",
    "* **Tasks are NOT thread**\n",
    "    * Julia Tasks are **not thread-safe**.\n",
    "    * See [Julia Multithreading](https://docs.julialang.org/en/stable/manual/parallel-computing/#Multi-Threading-(Experimental)-1)\n",
    "* Julia tasks are **green threads (coroutines)**\n",
    "    * Tasks do **not** spam over different CPUs\n",
    "    * Julia scheduler decides which tasks runs at a given time\n",
    "    * Tasks can yield, notify, schedule, etc.\n",
    "* Useful when you need concurrency programming with no actual parallelization (<span style=\"color:red\">**pseudo-parallelization**</span> instead)\n",
    "    * Context change is very fast as tasks belongs to a single kernel thread\n",
    "\n",
    "\n",
    "### Macro `@task `\n",
    "\n",
    "* Optionally, you can create a task with macro [@task](https://docs.julialang.org/en/stable/stdlib/parallel/#Base.@task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [<span style=\"color:red\">Distributed Arrays</span>](https://github.com/JuliaParallel/DistributedArrays.jl)\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "* **Distributed Memory** model\n",
    "    * A multi-dimensional array stored at distributed processes\n",
    "    * No shared memory\n",
    "* **Dense Arrays**\n",
    "    * E.g., non-sparse matrices\n",
    "* All processes know about DistributedArrays\n",
    "* **Only metadata is exchanged**\n",
    "    * Each process holds a part of data the whole DistributedArray\n",
    "\n",
    "```\n",
    "2 2 2 2\n",
    "3 3 3 3\n",
    "4 4 4 4\n",
    "5 5 5 5\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Int64,1}:\n",
       " 2\n",
       " 3"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mCloning cache of DistributedArrays from https://github.com/JuliaParallel/DistributedArrays.jl.git\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mCloning cache of Primes from https://github.com/JuliaMath/Primes.jl.git\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mInstalling DistributedArrays v0.4.0\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mInstalling Primes v0.3.0\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mPackage database updated\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mMETADATA is out-of-date — you may not have the latest version of DistributedArrays\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mUse `Pkg.update()` to get the latest versions of your packages\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "Pkg.add(\"DistributedArrays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere using DistributedArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DistributedArrays\n",
    "\n",
    "Use `Array` constructors with **d** at the beginning:\n",
    "\n",
    "    dzeros(100,100,10)\n",
    "    dones(100,100,10)\n",
    "    drand(100,100,10)\n",
    "    drandn(100,100,10)\n",
    "    dfill(x,100,100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element DistributedArrays.DArray{Float64,1,Array{Float64,1}}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dzeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = d[4]\n",
    "typeof(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating from an `Array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element DistributedArrays.DArray{Float64,1,Array{Float64,1}}:\n",
       " 0.321366\n",
       " 0.236102\n",
       " 0.214213\n",
       " 0.756046\n",
       " 0.541592\n",
       " 0.11205 \n",
       " 0.795864\n",
       " 0.312882\n",
       " 0.110152\n",
       " 0.350713"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = rand(10)\n",
    "darr = distribute(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting `DistributedArrays` to `Array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       " 0.321366\n",
       " 0.236102\n",
       " 0.214213\n",
       " 0.756046\n",
       " 0.541592\n",
       " 0.11205 \n",
       " 0.795864\n",
       " 0.312882\n",
       " 0.110152\n",
       " 0.350713"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2 = convert(Array, darr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array{Float64,1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof(arr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting information on where DistributedArrays are stored\n",
    "\n",
    "* **`localpart(a::DArray)`** obtains the locally-stored portion of a  DArray.\n",
    "\n",
    "* **`localindexes(a::DArray)`** gives a tuple of the index ranges owned by the local process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0-element Array{Float64,1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "localpart(darr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 2:\t(1:3,)\n",
      "\tFrom worker 3:\t(4:5,)\n",
      "\tFrom worker 4:\t(6:7,)\n",
      "\tFrom worker 5:\t(8:10,)\n"
     ]
    }
   ],
   "source": [
    "for w in workers()\n",
    "    @spawnat w println(localindexes(darr))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 2:\t[0.321366, 0.236102, 0.214213]\n",
      "\tFrom worker 4:\t[0.11205, 0.795864]\n",
      "\tFrom worker 5:\t[0.312882, 0.110152, 0.350713]\n",
      "\tFrom worker 3:\t[0.756046, 0.541592]\n"
     ]
    }
   ],
   "source": [
    "for w in workers()\n",
    "    @spawnat w println(localpart(darr))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d.cuts = Array{Int64,1}[[1, 4, 6, 8, 11]]\n",
      "d.dims = (10,)\n",
      "d.id = (1, 1)\n",
      "d.indexes = Tuple{UnitRange{Int64}}[(1:3,), (4:5,), (6:7,), (8:10,)]\n",
      "d.localpart = Nullable{Array{Float64,1}}(Float64[])\n",
      "d.pids = [2, 3, 4, 5]\n",
      "d.release = true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@show d.cuts\n",
    "@show d.dims\n",
    "@show d.id\n",
    "@show d.indexes\n",
    "@show d.localpart\n",
    "@show d.pids\n",
    "@show d.release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[1mD\u001b[22m\u001b[1mi\u001b[22m\u001b[1ms\u001b[22m\u001b[1mt\u001b[22m\u001b[1mr\u001b[22m\u001b[1mi\u001b[22m\u001b[1mb\u001b[22m\u001b[1mu\u001b[22m\u001b[1mt\u001b[22m\u001b[1me\u001b[22m\u001b[1md\u001b[22m\u001b[1mA\u001b[22m\u001b[1mr\u001b[22m\u001b[1mr\u001b[22m\u001b[1ma\u001b[22m\u001b[1my\u001b[22m\u001b[1ms\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "No documentation found.\n",
       "\n",
       "Displaying the `README.md` for the module instead.\n",
       "\n",
       "---\n",
       "\n",
       "# DistributedArrays.jl\n",
       "\n",
       "[![Build Status](https://travis-ci.org/JuliaParallel/DistributedArrays.jl.svg?branch=master)](https://travis-ci.org/JuliaParallel/DistributedArrays.jl) [![Coverage Status](https://coveralls.io/repos/github/JuliaParallel/DistributedArrays.jl/badge.svg?branch=master)](https://coveralls.io/github/JuliaParallel/DistributedArrays.jl?branch=master) [![codecov](https://codecov.io/gh/JuliaParallel/DistributedArrays.jl/branch/master/graph/badge.svg)](https://codecov.io/gh/JuliaParallel/DistributedArrays.jl)\n",
       "\n",
       "Distributed Arrays for Julia\n",
       "\n",
       "***NOTE*** Distributed Arrays will only work on Julia v0.4.0 or later.\n",
       "\n",
       "`DArray`s have been removed from Julia Base library in v0.4 so it is now necessary to import the `DistributedArrays` package on all spawned processes.\n",
       "\n",
       "```julia\n",
       "@everywhere using DistributedArrays\n",
       "```\n",
       "\n",
       "## Distributed Arrays\n",
       "\n",
       "Large computations are often organized around large arrays of data. In these cases, a particularly natural way to obtain parallelism is to distribute arrays among several processes. This combines the memory resources of multiple machines, allowing use of arrays too large to fit on one machine. Each process operates on the part of the array it owns, providing a ready answer to the question of how a program should be divided among machines.\n",
       "\n",
       "Julia distributed arrays are implemented by the `DArray` type. A `DArray` has an element type and dimensions just like an `Array`. A `DArray` can also use arbitrary array-like types to represent the local chunks that store actual data. The data in a `DArray` is distributed by dividing the index space into some number of blocks in each dimension.\n",
       "\n",
       "Common kinds of arrays can be constructed with functions beginning with `d`:\n",
       "\n",
       "```julia\n",
       "    dzeros(100,100,10)\n",
       "    dones(100,100,10)\n",
       "    drand(100,100,10)\n",
       "    drandn(100,100,10)\n",
       "    dfill(x,100,100,10)\n",
       "```\n",
       "\n",
       "In the last case, each element will be initialized to the specified value `x`. These functions automatically pick a distribution for you. For more control, you can specify which processes to use, and how the data should be distributed:\n",
       "\n",
       "```julia\n",
       "    dzeros((100,100), workers()[1:4], [1,4])\n",
       "```\n",
       "\n",
       "The second argument specifies that the array should be created on the first four workers. When dividing data among a large number of processes, one often sees diminishing returns in performance. Placing `DArray`\\ s on a subset of processes allows multiple `DArray` computations to happen at once, with a higher ratio of work to communication on each process.\n",
       "\n",
       "The third argument specifies a distribution; the nth element of this array specifies how many pieces dimension n should be divided into. In this example the first dimension will not be divided, and the second dimension will be divided into 4 pieces. Therefore each local chunk will be of size `(100,25)`. Note that the product of the distribution array must equal the number of processes.\n",
       "\n",
       "  * `distribute(a::Array)` converts a local array to a distributed array.\n",
       "  * `localpart(d::DArray)` obtains the locally-stored portion\n",
       "\n",
       "of a  `DArray`.\n",
       "\n",
       "  * Localparts can be retrived and set via the indexing syntax too.\n",
       "\n",
       "Indexing via symbols is used for this, specifically symbols `:L`,`:LP`,`:l`,`:lp` which are all equivalent. For example, `d[:L]` returns the localpart of `d` while `d[:L]=v` sets `v` as the localpart of `d`.\n",
       "\n",
       "  * `localindexes(a::DArray)` gives a tuple of the index ranges owned by the\n",
       "\n",
       "local process.\n",
       "\n",
       "  * `convert(Array, a::DArray)` brings all the data to the local process.\n",
       "\n",
       "Indexing a `DArray` (square brackets) with ranges of indexes always creates a `SubArray`, not copying any data.\n",
       "\n",
       "## Constructing Distributed Arrays\n",
       "\n",
       "The primitive `DArray` constructor has the following somewhat elaborate signature:\n",
       "\n",
       "```julia\n",
       "    DArray(init, dims[, procs, dist])\n",
       "```\n",
       "\n",
       "`init` is a function that accepts a tuple of index ranges. This function should allocate a local chunk of the distributed array and initialize it for the specified indices. `dims` is the overall size of the distributed array. `procs` optionally specifies a vector of process IDs to use. `dist` is an integer vector specifying how many chunks the distributed array should be divided into in each dimension.\n",
       "\n",
       "The last two arguments are optional, and defaults will be used if they are omitted.\n",
       "\n",
       "As an example, here is how to turn the local array constructor `fill` into a distributed array constructor:\n",
       "\n",
       "```julia\n",
       "    dfill(v, args...) = DArray(I->fill(v, map(length,I)), args...)\n",
       "```\n",
       "\n",
       "In this case the `init` function only needs to call `fill` with the dimensions of the local piece it is creating.\n",
       "\n",
       "`DArray`s can also be constructed from multidimensional `Array` comprehensions with the `@DArray` macro syntax.  This syntax is just sugar for the primitive `DArray` constructor:\n",
       "\n",
       "```julia\n",
       "julia> [i+j for i = 1:5, j = 1:5]\n",
       "5x5 Array{Int64,2}:\n",
       " 2  3  4  5   6\n",
       " 3  4  5  6   7\n",
       " 4  5  6  7   8\n",
       " 5  6  7  8   9\n",
       " 6  7  8  9  10\n",
       "\n",
       "julia> @DArray [i+j for i = 1:5, j = 1:5]\n",
       "5x5 DistributedArrays.DArray{Int64,2,Array{Int64,2}}:\n",
       " 2  3  4  5   6\n",
       " 3  4  5  6   7\n",
       " 4  5  6  7   8\n",
       " 5  6  7  8   9\n",
       " 6  7  8  9  10\n",
       "```\n",
       "\n",
       "## Distributed Array Operations\n",
       "\n",
       "At this time, distributed arrays do not have much functionality. Their major utility is allowing communication to be done via array indexing, which is convenient for many problems. As an example, consider implementing the \"life\" cellular automaton, where each cell in a grid is updated according to its neighboring cells. To compute a chunk of the result of one iteration, each process needs the immediate neighbor cells of its local chunk. The following code accomplishes this::\n",
       "\n",
       "```julia\n",
       "    function life_step(d::DArray)\n",
       "        DArray(size(d),procs(d)) do I\n",
       "            top   = mod(first(I[1])-2,size(d,1))+1\n",
       "            bot   = mod( last(I[1])  ,size(d,1))+1\n",
       "            left  = mod(first(I[2])-2,size(d,2))+1\n",
       "            right = mod( last(I[2])  ,size(d,2))+1\n",
       "\n",
       "            old = Array(Bool, length(I[1])+2, length(I[2])+2)\n",
       "            old[1      , 1      ] = d[top , left]   # left side\n",
       "            old[2:end-1, 1      ] = d[I[1], left]\n",
       "            old[end    , 1      ] = d[bot , left]\n",
       "            old[1      , 2:end-1] = d[top , I[2]]\n",
       "            old[2:end-1, 2:end-1] = d[I[1], I[2]]   # middle\n",
       "            old[end    , 2:end-1] = d[bot , I[2]]\n",
       "            old[1      , end    ] = d[top , right]  # right side\n",
       "            old[2:end-1, end    ] = d[I[1], right]\n",
       "            old[end    , end    ] = d[bot , right]\n",
       "\n",
       "            life_rule(old)\n",
       "        end\n",
       "    end\n",
       "```\n",
       "\n",
       "As you can see, we use a series of indexing expressions to fetch data into a local array `old`. Note that the `do` block syntax is convenient for passing `init` functions to the `DArray` constructor. Next, the serial function `life_rule` is called to apply the update rules to the data, yielding the needed `DArray` chunk. Nothing about `life_rule` is `DArray`\\ -specific, but we list it here for completeness::\n",
       "\n",
       "```julia\n",
       "    function life_rule(old)\n",
       "        m, n = size(old)\n",
       "        new = similar(old, m-2, n-2)\n",
       "        for j = 2:n-1\n",
       "            for i = 2:m-1\n",
       "                nc = +(old[i-1,j-1], old[i-1,j], old[i-1,j+1],\n",
       "                       old[i  ,j-1],             old[i  ,j+1],\n",
       "                       old[i+1,j-1], old[i+1,j], old[i+1,j+1])\n",
       "                new[i-1,j-1] = (nc == 3 || nc == 2 && old[i,j])\n",
       "            end\n",
       "        end\n",
       "        new\n",
       "    end\n",
       "```\n",
       "\n",
       "## Numerical Results of Distributed Computations\n",
       "\n",
       "Floating point arithmetic is not associative and this comes up when performing distributed computations over `DArray`s.  All `DArray` operations are performed over the `localpart` chunks and then aggregated. The change in ordering of the operations will change the numeric result as seen in this simple example:\n",
       "\n",
       "```julia\n",
       "julia> addprocs(8);\n",
       "\n",
       "julia> @everywhere using DistributedArrays\n",
       "\n",
       "julia> A = fill(1.1, (100,100));\n",
       "\n",
       "julia> sum(A)\n",
       "11000.000000000013\n",
       "\n",
       "julia> DA = distribute(A);\n",
       "\n",
       "julia> sum(DA)\n",
       "11000.000000000127\n",
       "\n",
       "julia> sum(A) == sum(DA)\n",
       "false\n",
       "```\n",
       "\n",
       "The ultimate ordering of operations will be dependent on how the Array is distributed.\n",
       "\n",
       "## Garbage Collection and DArrays\n",
       "\n",
       "When a DArray is constructed (typically on the master process), the returned DArray objects stores information on how the array is distributed, which procesor holds which indexes and so on. When the DArray object on the master process is garbage collected, all particpating workers are notified and localparts of the DArray freed on each worker.\n",
       "\n",
       "Since the size of the DArray object itself is small, a problem arises as `gc` on the master faces no memory pressure to collect the DArray immediately. This results in a delay of the memory being released on the participating workers.\n",
       "\n",
       "Therefore it is highly recommended to explcitly call `close(d::DArray)` as soon as user code has finished working with the distributed array.\n",
       "\n",
       "It is also important to note that the localparts of the DArray is collected from all particpating workers when the DArray object on the process creating the DArray is collected. It is therefore important to maintain a reference to a DArray object on the creating process for as long as it is being computed upon.\n",
       "\n",
       "`d_closeall()` is another useful function to manage distributed memory. It releases all darrays created from the calling process, including any temporaries created during computation.\n",
       "\n",
       "## Working with distributed non-array data (requires Julia 0.6)\n",
       "\n",
       "The function `ddata(;T::Type=Any, init::Function=I->nothing, pids=workers(), data::Vector=[])` can be used to created a distributed vector whose localparts need not be Arrays.\n",
       "\n",
       "It returns a `DArray{T,1,T}`, i.e., the element type and localtype of the array are the same.\n",
       "\n",
       "`ddata()` constructs a distributed vector of length `nworkers()` where each localpart can hold any value, initially initialized to `nothing`.\n",
       "\n",
       "Argument `data` if supplied is distributed over the `pids`. `length(data)` must be a multiple of `length(pids)`. If the multiple is 1, returns a `DArray{T,1,T}` where T is `eltype(data)`. If the multiple is greater than 1, returns a `DArray{T,1,Array{T,1}}`, i.e., it is equivalent to calling `distribute(data)`.\n",
       "\n",
       "`gather{T}(d::DArray{T,1,T})` returns an Array{T,1} consisting of all distributed elements of `d`\n",
       "\n",
       "Given a `DArray{T,1,T}` object `d`, `d[:L]` returns the localpart on a worker. `d[i]` returns the `localpart` on the ith worker that `d` is distributed over.\n",
       "\n",
       "## SPMD Mode (An MPI Style SPMD mode with MPI like primitives, requires Julia 0.6)\n",
       "\n",
       "SPMD, i.e., a Single Program Multiple Data mode is implemented by submodule `DistributedArrays.SPMD`. In this mode the same function is executed in parallel on all participating nodes. This is a typical style of MPI programs where the same program is executed on all processors. A basic subset of MPI-like primitives are currently supported. As a programming model it should be familiar to folks with an MPI background.\n",
       "\n",
       "The same block of code is executed concurrently on all workers using the `spmd` function.\n",
       "\n",
       "```\n",
       "# define foo() on all workers\n",
       "@everywhere function foo(arg1, arg2)\n",
       "    ....\n",
       "end\n",
       "\n",
       "# call foo() everywhere using the `spmd` function\n",
       "d_in=DArray(.....)\n",
       "d_out=ddata()\n",
       "spmd(foo,d_in,d_out; pids=workers()) # executes on all workers\n",
       "```\n",
       "\n",
       "`spmd` is defined as `spmd(f, args...; pids=procs(), context=nothing)`\n",
       "\n",
       "`args` is one or more arguments to be passed to `f`. `pids` identifies the workers that `f` needs to be run on. `context` identifies a run context, which is explained later.\n",
       "\n",
       "The following primitives can be used in SPMD mode.\n",
       "\n",
       "  * `sendto(pid, data; tag=nothing)` - sends `data` to `pid`\n",
       "  * `recvfrom(pid; tag=nothing)` - receives data from `pid`\n",
       "  * `recvfrom_any(; tag=nothing)` - receives data from any `pid`\n",
       "  * `barrier(;pids=procs(), tag=nothing)` - all tasks wait and then proceeed\n",
       "  * `bcast(data, pid; tag=nothing, pids=procs())` - broadcasts the same data over `pids` from `pid`\n",
       "  * `scatter(x, pid; tag=nothing, pids=procs())` - distributes `x` over `pids` from `pid`\n",
       "  * `gather(x, pid; tag=nothing, pids=procs())` - collects data from `pids` onto worker `pid`\n",
       "\n",
       "Tag `tag` should be used to differentiate between consecutive calls of the same type, for example, consecutive `bcast` calls.\n",
       "\n",
       "`spmd` and spmd related functions are defined in submodule `DistributedArrays.SPMD`. You will need to import it explcitly, or prefix functions that can can only be used in spmd mode with `SPMD.`, for example, `SPMD.sendto`.\n",
       "\n",
       "## Example\n",
       "\n",
       "This toy example exchanges data with each of its neighbors `n` times.\n",
       "\n",
       "```\n",
       "using DistributedArrays\n",
       "addprocs(8)\n",
       "@everywhere importall DistributedArrays\n",
       "@everywhere importall DistributedArrays.SPMD\n",
       "\n",
       "d_in=d=DArray(I->fill(myid(), (map(length,I)...)), (nworkers(), 2), workers(), [nworkers(),1])\n",
       "d_out=ddata()\n",
       "\n",
       "# define the function everywhere\n",
       "@everywhere function foo_spmd(d_in, d_out, n)\n",
       "    pids = sort(vec(procs(d_in)))\n",
       "    pididx = findfirst(pids, myid())\n",
       "    mylp = d_in[:L]\n",
       "    localsum = 0\n",
       "\n",
       "    # Have each worker exchange data with its neighbors\n",
       "    n_pididx = pididx+1 > length(pids) ? 1 : pididx+1\n",
       "    p_pididx = pididx-1 < 1 ? length(pids) : pididx-1\n",
       "\n",
       "    for i in 1:n\n",
       "        sendto(pids[n_pididx], mylp[2])\n",
       "        sendto(pids[p_pididx], mylp[1])\n",
       "\n",
       "        mylp[2] = recvfrom(pids[p_pididx])\n",
       "        mylp[1] = recvfrom(pids[n_pididx])\n",
       "\n",
       "        barrier(;pids=pids)\n",
       "        localsum = localsum + mylp[1] + mylp[2]\n",
       "    end\n",
       "\n",
       "    # finally store the sum in d_out\n",
       "    d_out[:L] = localsum\n",
       "end\n",
       "\n",
       "# run foo_spmd on all workers\n",
       "spmd(foo_spmd, d_in, d_out, 10)\n",
       "\n",
       "# print values of d_in and d_out after the run\n",
       "println(d_in)\n",
       "println(d_out)\n",
       "```\n",
       "\n",
       "## SPMD Context\n",
       "\n",
       "Each SPMD run is implictly executed in a different context. This allows for multiple `spmd` calls to be active at the same time. A SPMD context can be explicitly specified via keyword arg `context` to `spmd`.\n",
       "\n",
       "`context(pids=procs())` returns a new SPMD context.\n",
       "\n",
       "A SPMD context also provides a context local storage, a dict, which can be used to store key-value pairs between spmd runs under the same context.\n",
       "\n",
       "`context_local_storage()` returns the dictionary associated with the context.\n",
       "\n",
       "NOTE: Implicitly defined contexts, i.e., `spmd` calls without specifying a `context` create a context which live only for the duration of the call. Explictly created context objects can be released early by calling `close(ctxt::SPMDContext)`. This will release the local storage dictionaries on all participating `pids`. Else they will be released when the context object is gc'ed on the node that created it.\n",
       "\n",
       "## Nested `spmd` calls\n",
       "\n",
       "As `spmd` executes the the specified function on all participating nodes, we need to be careful with nesting `spmd` calls.\n",
       "\n",
       "An example of an unsafe(wrong) way:\n",
       "\n",
       "```\n",
       "function foo(.....)\n",
       "    ......\n",
       "    spmd(bar, ......)\n",
       "    ......\n",
       "end\n",
       "\n",
       "function bar(....)\n",
       "    ......\n",
       "    spmd(baz, ......)\n",
       "    ......\n",
       "end\n",
       "\n",
       "spmd(foo,....)\n",
       "```\n",
       "\n",
       "In the above example, `foo`, `bar` and `baz` are all functions wishing to leverage distributed computation. However, they themselves may be currenty part of a `spmd` call. A safe way to handle such a scenario is to only drive parallel computation from the master process.\n",
       "\n",
       "The correct way (only have the driver process initiate `spmd` calls):\n",
       "\n",
       "```\n",
       "function foo()\n",
       "    ......\n",
       "    myid()==1 && spmd(bar, ......)\n",
       "    ......\n",
       "end\n",
       "\n",
       "function bar()\n",
       "    ......\n",
       "    myid()==1 && spmd(baz, ......)\n",
       "    ......\n",
       "end\n",
       "\n",
       "spmd(foo,....)\n",
       "```\n",
       "\n",
       "This is also true of functions which automatically distribute computation on DArrays.\n",
       "\n",
       "```\n",
       "function foo(d::DArray)\n",
       "    ......\n",
       "    myid()==1 && map!(bar, d)\n",
       "    ......\n",
       "end\n",
       "spmd(foo,....)\n",
       "```\n",
       "\n",
       "Without the `myid()` check, the `spmd` call to `foo` would execute `map!` from all nodes, which is not what we probably want.\n",
       "\n",
       "Similarly `@everywhere` from within a SPMD run should also be driven from the master node only.\n"
      ],
      "text/plain": [
       "No documentation found.\n",
       "\n",
       "Displaying the `README.md` for the module instead.\n",
       "\n",
       "---\n",
       "\n",
       "# DistributedArrays.jl\n",
       "\n",
       "[![Build Status](https://travis-ci.org/JuliaParallel/DistributedArrays.jl.svg?branch=master)](https://travis-ci.org/JuliaParallel/DistributedArrays.jl) [![Coverage Status](https://coveralls.io/repos/github/JuliaParallel/DistributedArrays.jl/badge.svg?branch=master)](https://coveralls.io/github/JuliaParallel/DistributedArrays.jl?branch=master) [![codecov](https://codecov.io/gh/JuliaParallel/DistributedArrays.jl/branch/master/graph/badge.svg)](https://codecov.io/gh/JuliaParallel/DistributedArrays.jl)\n",
       "\n",
       "Distributed Arrays for Julia\n",
       "\n",
       "***NOTE*** Distributed Arrays will only work on Julia v0.4.0 or later.\n",
       "\n",
       "`DArray`s have been removed from Julia Base library in v0.4 so it is now necessary to import the `DistributedArrays` package on all spawned processes.\n",
       "\n",
       "```julia\n",
       "@everywhere using DistributedArrays\n",
       "```\n",
       "\n",
       "## Distributed Arrays\n",
       "\n",
       "Large computations are often organized around large arrays of data. In these cases, a particularly natural way to obtain parallelism is to distribute arrays among several processes. This combines the memory resources of multiple machines, allowing use of arrays too large to fit on one machine. Each process operates on the part of the array it owns, providing a ready answer to the question of how a program should be divided among machines.\n",
       "\n",
       "Julia distributed arrays are implemented by the `DArray` type. A `DArray` has an element type and dimensions just like an `Array`. A `DArray` can also use arbitrary array-like types to represent the local chunks that store actual data. The data in a `DArray` is distributed by dividing the index space into some number of blocks in each dimension.\n",
       "\n",
       "Common kinds of arrays can be constructed with functions beginning with `d`:\n",
       "\n",
       "```julia\n",
       "    dzeros(100,100,10)\n",
       "    dones(100,100,10)\n",
       "    drand(100,100,10)\n",
       "    drandn(100,100,10)\n",
       "    dfill(x,100,100,10)\n",
       "```\n",
       "\n",
       "In the last case, each element will be initialized to the specified value `x`. These functions automatically pick a distribution for you. For more control, you can specify which processes to use, and how the data should be distributed:\n",
       "\n",
       "```julia\n",
       "    dzeros((100,100), workers()[1:4], [1,4])\n",
       "```\n",
       "\n",
       "The second argument specifies that the array should be created on the first four workers. When dividing data among a large number of processes, one often sees diminishing returns in performance. Placing `DArray`\\ s on a subset of processes allows multiple `DArray` computations to happen at once, with a higher ratio of work to communication on each process.\n",
       "\n",
       "The third argument specifies a distribution; the nth element of this array specifies how many pieces dimension n should be divided into. In this example the first dimension will not be divided, and the second dimension will be divided into 4 pieces. Therefore each local chunk will be of size `(100,25)`. Note that the product of the distribution array must equal the number of processes.\n",
       "\n",
       "  * `distribute(a::Array)` converts a local array to a distributed array.\n",
       "  * `localpart(d::DArray)` obtains the locally-stored portion\n",
       "\n",
       "of a  `DArray`.\n",
       "\n",
       "  * Localparts can be retrived and set via the indexing syntax too.\n",
       "\n",
       "Indexing via symbols is used for this, specifically symbols `:L`,`:LP`,`:l`,`:lp` which are all equivalent. For example, `d[:L]` returns the localpart of `d` while `d[:L]=v` sets `v` as the localpart of `d`.\n",
       "\n",
       "  * `localindexes(a::DArray)` gives a tuple of the index ranges owned by the\n",
       "\n",
       "local process.\n",
       "\n",
       "  * `convert(Array, a::DArray)` brings all the data to the local process.\n",
       "\n",
       "Indexing a `DArray` (square brackets) with ranges of indexes always creates a `SubArray`, not copying any data.\n",
       "\n",
       "## Constructing Distributed Arrays\n",
       "\n",
       "The primitive `DArray` constructor has the following somewhat elaborate signature:\n",
       "\n",
       "```julia\n",
       "    DArray(init, dims[, procs, dist])\n",
       "```\n",
       "\n",
       "`init` is a function that accepts a tuple of index ranges. This function should allocate a local chunk of the distributed array and initialize it for the specified indices. `dims` is the overall size of the distributed array. `procs` optionally specifies a vector of process IDs to use. `dist` is an integer vector specifying how many chunks the distributed array should be divided into in each dimension.\n",
       "\n",
       "The last two arguments are optional, and defaults will be used if they are omitted.\n",
       "\n",
       "As an example, here is how to turn the local array constructor `fill` into a distributed array constructor:\n",
       "\n",
       "```julia\n",
       "    dfill(v, args...) = DArray(I->fill(v, map(length,I)), args...)\n",
       "```\n",
       "\n",
       "In this case the `init` function only needs to call `fill` with the dimensions of the local piece it is creating.\n",
       "\n",
       "`DArray`s can also be constructed from multidimensional `Array` comprehensions with the `@DArray` macro syntax.  This syntax is just sugar for the primitive `DArray` constructor:\n",
       "\n",
       "```julia\n",
       "julia> [i+j for i = 1:5, j = 1:5]\n",
       "5x5 Array{Int64,2}:\n",
       " 2  3  4  5   6\n",
       " 3  4  5  6   7\n",
       " 4  5  6  7   8\n",
       " 5  6  7  8   9\n",
       " 6  7  8  9  10\n",
       "\n",
       "julia> @DArray [i+j for i = 1:5, j = 1:5]\n",
       "5x5 DistributedArrays.DArray{Int64,2,Array{Int64,2}}:\n",
       " 2  3  4  5   6\n",
       " 3  4  5  6   7\n",
       " 4  5  6  7   8\n",
       " 5  6  7  8   9\n",
       " 6  7  8  9  10\n",
       "```\n",
       "\n",
       "## Distributed Array Operations\n",
       "\n",
       "At this time, distributed arrays do not have much functionality. Their major utility is allowing communication to be done via array indexing, which is convenient for many problems. As an example, consider implementing the \"life\" cellular automaton, where each cell in a grid is updated according to its neighboring cells. To compute a chunk of the result of one iteration, each process needs the immediate neighbor cells of its local chunk. The following code accomplishes this::\n",
       "\n",
       "```julia\n",
       "    function life_step(d::DArray)\n",
       "        DArray(size(d),procs(d)) do I\n",
       "            top   = mod(first(I[1])-2,size(d,1))+1\n",
       "            bot   = mod( last(I[1])  ,size(d,1))+1\n",
       "            left  = mod(first(I[2])-2,size(d,2))+1\n",
       "            right = mod( last(I[2])  ,size(d,2))+1\n",
       "\n",
       "            old = Array(Bool, length(I[1])+2, length(I[2])+2)\n",
       "            old[1      , 1      ] = d[top , left]   # left side\n",
       "            old[2:end-1, 1      ] = d[I[1], left]\n",
       "            old[end    , 1      ] = d[bot , left]\n",
       "            old[1      , 2:end-1] = d[top , I[2]]\n",
       "            old[2:end-1, 2:end-1] = d[I[1], I[2]]   # middle\n",
       "            old[end    , 2:end-1] = d[bot , I[2]]\n",
       "            old[1      , end    ] = d[top , right]  # right side\n",
       "            old[2:end-1, end    ] = d[I[1], right]\n",
       "            old[end    , end    ] = d[bot , right]\n",
       "\n",
       "            life_rule(old)\n",
       "        end\n",
       "    end\n",
       "```\n",
       "\n",
       "As you can see, we use a series of indexing expressions to fetch data into a local array `old`. Note that the `do` block syntax is convenient for passing `init` functions to the `DArray` constructor. Next, the serial function `life_rule` is called to apply the update rules to the data, yielding the needed `DArray` chunk. Nothing about `life_rule` is `DArray`\\ -specific, but we list it here for completeness::\n",
       "\n",
       "```julia\n",
       "    function life_rule(old)\n",
       "        m, n = size(old)\n",
       "        new = similar(old, m-2, n-2)\n",
       "        for j = 2:n-1\n",
       "            for i = 2:m-1\n",
       "                nc = +(old[i-1,j-1], old[i-1,j], old[i-1,j+1],\n",
       "                       old[i  ,j-1],             old[i  ,j+1],\n",
       "                       old[i+1,j-1], old[i+1,j], old[i+1,j+1])\n",
       "                new[i-1,j-1] = (nc == 3 || nc == 2 && old[i,j])\n",
       "            end\n",
       "        end\n",
       "        new\n",
       "    end\n",
       "```\n",
       "\n",
       "## Numerical Results of Distributed Computations\n",
       "\n",
       "Floating point arithmetic is not associative and this comes up when performing distributed computations over `DArray`s.  All `DArray` operations are performed over the `localpart` chunks and then aggregated. The change in ordering of the operations will change the numeric result as seen in this simple example:\n",
       "\n",
       "```julia\n",
       "julia> addprocs(8);\n",
       "\n",
       "julia> @everywhere using DistributedArrays\n",
       "\n",
       "julia> A = fill(1.1, (100,100));\n",
       "\n",
       "julia> sum(A)\n",
       "11000.000000000013\n",
       "\n",
       "julia> DA = distribute(A);\n",
       "\n",
       "julia> sum(DA)\n",
       "11000.000000000127\n",
       "\n",
       "julia> sum(A) == sum(DA)\n",
       "false\n",
       "```\n",
       "\n",
       "The ultimate ordering of operations will be dependent on how the Array is distributed.\n",
       "\n",
       "## Garbage Collection and DArrays\n",
       "\n",
       "When a DArray is constructed (typically on the master process), the returned DArray objects stores information on how the array is distributed, which procesor holds which indexes and so on. When the DArray object on the master process is garbage collected, all particpating workers are notified and localparts of the DArray freed on each worker.\n",
       "\n",
       "Since the size of the DArray object itself is small, a problem arises as `gc` on the master faces no memory pressure to collect the DArray immediately. This results in a delay of the memory being released on the participating workers.\n",
       "\n",
       "Therefore it is highly recommended to explcitly call `close(d::DArray)` as soon as user code has finished working with the distributed array.\n",
       "\n",
       "It is also important to note that the localparts of the DArray is collected from all particpating workers when the DArray object on the process creating the DArray is collected. It is therefore important to maintain a reference to a DArray object on the creating process for as long as it is being computed upon.\n",
       "\n",
       "`d_closeall()` is another useful function to manage distributed memory. It releases all darrays created from the calling process, including any temporaries created during computation.\n",
       "\n",
       "## Working with distributed non-array data (requires Julia 0.6)\n",
       "\n",
       "The function `ddata(;T::Type=Any, init::Function=I->nothing, pids=workers(), data::Vector=[])` can be used to created a distributed vector whose localparts need not be Arrays.\n",
       "\n",
       "It returns a `DArray{T,1,T}`, i.e., the element type and localtype of the array are the same.\n",
       "\n",
       "`ddata()` constructs a distributed vector of length `nworkers()` where each localpart can hold any value, initially initialized to `nothing`.\n",
       "\n",
       "Argument `data` if supplied is distributed over the `pids`. `length(data)` must be a multiple of `length(pids)`. If the multiple is 1, returns a `DArray{T,1,T}` where T is `eltype(data)`. If the multiple is greater than 1, returns a `DArray{T,1,Array{T,1}}`, i.e., it is equivalent to calling `distribute(data)`.\n",
       "\n",
       "`gather{T}(d::DArray{T,1,T})` returns an Array{T,1} consisting of all distributed elements of `d`\n",
       "\n",
       "Given a `DArray{T,1,T}` object `d`, `d[:L]` returns the localpart on a worker. `d[i]` returns the `localpart` on the ith worker that `d` is distributed over.\n",
       "\n",
       "## SPMD Mode (An MPI Style SPMD mode with MPI like primitives, requires Julia 0.6)\n",
       "\n",
       "SPMD, i.e., a Single Program Multiple Data mode is implemented by submodule `DistributedArrays.SPMD`. In this mode the same function is executed in parallel on all participating nodes. This is a typical style of MPI programs where the same program is executed on all processors. A basic subset of MPI-like primitives are currently supported. As a programming model it should be familiar to folks with an MPI background.\n",
       "\n",
       "The same block of code is executed concurrently on all workers using the `spmd` function.\n",
       "\n",
       "```\n",
       "# define foo() on all workers\n",
       "@everywhere function foo(arg1, arg2)\n",
       "    ....\n",
       "end\n",
       "\n",
       "# call foo() everywhere using the `spmd` function\n",
       "d_in=DArray(.....)\n",
       "d_out=ddata()\n",
       "spmd(foo,d_in,d_out; pids=workers()) # executes on all workers\n",
       "```\n",
       "\n",
       "`spmd` is defined as `spmd(f, args...; pids=procs(), context=nothing)`\n",
       "\n",
       "`args` is one or more arguments to be passed to `f`. `pids` identifies the workers that `f` needs to be run on. `context` identifies a run context, which is explained later.\n",
       "\n",
       "The following primitives can be used in SPMD mode.\n",
       "\n",
       "  * `sendto(pid, data; tag=nothing)` - sends `data` to `pid`\n",
       "  * `recvfrom(pid; tag=nothing)` - receives data from `pid`\n",
       "  * `recvfrom_any(; tag=nothing)` - receives data from any `pid`\n",
       "  * `barrier(;pids=procs(), tag=nothing)` - all tasks wait and then proceeed\n",
       "  * `bcast(data, pid; tag=nothing, pids=procs())` - broadcasts the same data over `pids` from `pid`\n",
       "  * `scatter(x, pid; tag=nothing, pids=procs())` - distributes `x` over `pids` from `pid`\n",
       "  * `gather(x, pid; tag=nothing, pids=procs())` - collects data from `pids` onto worker `pid`\n",
       "\n",
       "Tag `tag` should be used to differentiate between consecutive calls of the same type, for example, consecutive `bcast` calls.\n",
       "\n",
       "`spmd` and spmd related functions are defined in submodule `DistributedArrays.SPMD`. You will need to import it explcitly, or prefix functions that can can only be used in spmd mode with `SPMD.`, for example, `SPMD.sendto`.\n",
       "\n",
       "## Example\n",
       "\n",
       "This toy example exchanges data with each of its neighbors `n` times.\n",
       "\n",
       "```\n",
       "using DistributedArrays\n",
       "addprocs(8)\n",
       "@everywhere importall DistributedArrays\n",
       "@everywhere importall DistributedArrays.SPMD\n",
       "\n",
       "d_in=d=DArray(I->fill(myid(), (map(length,I)...)), (nworkers(), 2), workers(), [nworkers(),1])\n",
       "d_out=ddata()\n",
       "\n",
       "# define the function everywhere\n",
       "@everywhere function foo_spmd(d_in, d_out, n)\n",
       "    pids = sort(vec(procs(d_in)))\n",
       "    pididx = findfirst(pids, myid())\n",
       "    mylp = d_in[:L]\n",
       "    localsum = 0\n",
       "\n",
       "    # Have each worker exchange data with its neighbors\n",
       "    n_pididx = pididx+1 > length(pids) ? 1 : pididx+1\n",
       "    p_pididx = pididx-1 < 1 ? length(pids) : pididx-1\n",
       "\n",
       "    for i in 1:n\n",
       "        sendto(pids[n_pididx], mylp[2])\n",
       "        sendto(pids[p_pididx], mylp[1])\n",
       "\n",
       "        mylp[2] = recvfrom(pids[p_pididx])\n",
       "        mylp[1] = recvfrom(pids[n_pididx])\n",
       "\n",
       "        barrier(;pids=pids)\n",
       "        localsum = localsum + mylp[1] + mylp[2]\n",
       "    end\n",
       "\n",
       "    # finally store the sum in d_out\n",
       "    d_out[:L] = localsum\n",
       "end\n",
       "\n",
       "# run foo_spmd on all workers\n",
       "spmd(foo_spmd, d_in, d_out, 10)\n",
       "\n",
       "# print values of d_in and d_out after the run\n",
       "println(d_in)\n",
       "println(d_out)\n",
       "```\n",
       "\n",
       "## SPMD Context\n",
       "\n",
       "Each SPMD run is implictly executed in a different context. This allows for multiple `spmd` calls to be active at the same time. A SPMD context can be explicitly specified via keyword arg `context` to `spmd`.\n",
       "\n",
       "`context(pids=procs())` returns a new SPMD context.\n",
       "\n",
       "A SPMD context also provides a context local storage, a dict, which can be used to store key-value pairs between spmd runs under the same context.\n",
       "\n",
       "`context_local_storage()` returns the dictionary associated with the context.\n",
       "\n",
       "NOTE: Implicitly defined contexts, i.e., `spmd` calls without specifying a `context` create a context which live only for the duration of the call. Explictly created context objects can be released early by calling `close(ctxt::SPMDContext)`. This will release the local storage dictionaries on all participating `pids`. Else they will be released when the context object is gc'ed on the node that created it.\n",
       "\n",
       "## Nested `spmd` calls\n",
       "\n",
       "As `spmd` executes the the specified function on all participating nodes, we need to be careful with nesting `spmd` calls.\n",
       "\n",
       "An example of an unsafe(wrong) way:\n",
       "\n",
       "```\n",
       "function foo(.....)\n",
       "    ......\n",
       "    spmd(bar, ......)\n",
       "    ......\n",
       "end\n",
       "\n",
       "function bar(....)\n",
       "    ......\n",
       "    spmd(baz, ......)\n",
       "    ......\n",
       "end\n",
       "\n",
       "spmd(foo,....)\n",
       "```\n",
       "\n",
       "In the above example, `foo`, `bar` and `baz` are all functions wishing to leverage distributed computation. However, they themselves may be currenty part of a `spmd` call. A safe way to handle such a scenario is to only drive parallel computation from the master process.\n",
       "\n",
       "The correct way (only have the driver process initiate `spmd` calls):\n",
       "\n",
       "```\n",
       "function foo()\n",
       "    ......\n",
       "    myid()==1 && spmd(bar, ......)\n",
       "    ......\n",
       "end\n",
       "\n",
       "function bar()\n",
       "    ......\n",
       "    myid()==1 && spmd(baz, ......)\n",
       "    ......\n",
       "end\n",
       "\n",
       "spmd(foo,....)\n",
       "```\n",
       "\n",
       "This is also true of functions which automatically distribute computation on DArrays.\n",
       "\n",
       "```\n",
       "function foo(d::DArray)\n",
       "    ......\n",
       "    myid()==1 && map!(bar, d)\n",
       "    ......\n",
       "end\n",
       "spmd(foo,....)\n",
       "```\n",
       "\n",
       "Without the `myid()` check, the `spmd` call to `foo` would execute `map!` from all nodes, which is not what we probably want.\n",
       "\n",
       "Similarly `@everywhere` from within a SPMD run should also be driven from the master node only.\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?DistributedArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced topics in DArray\n",
    "\n",
    "### Customizing DistributedArrays creation\n",
    "\n",
    "Let's add 2 more workers and customize the DistributedArray creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#addprocs(2)\n",
    "#@everywhere using DistributedArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×4 DistributedArrays.DArray{Float64,2,Array{Float64,2}}:\n",
       " 0.492231   0.586387  0.786584  0.793067\n",
       " 0.35221    0.754233  0.963894  0.387072\n",
       " 0.427639   0.889434  0.980305  0.678837\n",
       " 0.956464   0.37371   0.538015  0.56272 \n",
       " 0.351742   0.763186  0.901514  0.241628\n",
       " 0.178048   0.378676  0.243198  0.688606\n",
       " 0.0295433  0.572676  0.597314  0.387573\n",
       " 0.385754   0.8526    0.588251  0.547671\n",
       " 0.153816   0.615701  0.407551  0.772227\n",
       " 0.966094   0.616491  0.199936  0.975261"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "darray = drand((10,4), workers()[1:4], [1,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 4:\t(1:10, 3:3)\n",
      "\tFrom worker 2:\t(1:10, 1:1)\n",
      "\tFrom worker 5:\t(1:10, 4:4)\n",
      "\tFrom worker 3:\t(1:10, 2:2)\n"
     ]
    }
   ],
   "source": [
    "for w in workers()\n",
    "    @spawnat w println(localindexes(darray))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also know which processes hold `darray`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×4 Array{Int64,2}:\n",
       " 2  3  4  5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "darray.pids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Or we can know the splitting indexes of `darray`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×4 Array{Tuple{UnitRange{Int64},UnitRange{Int64}},2}:\n",
       " (1:10, 1:1)  (1:10, 2:2)  (1:10, 3:3)  (1:10, 4:4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "darray.indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From a process, we can know the indexes it holds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 4:\tlocalindexes(darray) = (1:10, 3:3)\n",
      "\tFrom worker 3:\tlocalindexes(darray) = (1:10, 2:2)\n",
      "\tFrom worker 2:\tlocalindexes(darray) = (1:10, 1:1)\n",
      "\tFrom worker 5:\tlocalindexes(darray) = (1:10, 4:4)\n"
     ]
    }
   ],
   "source": [
    "for w in workers()\n",
    "    @spawnat w @show localindexes(darray)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can get any data of `darray` from any process, for instance, from the master:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3517419161884381"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "darray[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also access `darray` from any process such as worker 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3517419161884381"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch(@spawnat 4 darray[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed reduce using DistributedArrays \n",
    "* From Jeff Bezanson's Tutorial on [Parallel and Distributed Computing with Julia](https://www.youtube.com/watch?v=JoRn4ryMclc)\n",
    "\n",
    "* If we wanted to sum `darray` locally, we can simply call `+(darray)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×4 DistributedArrays.DArray{Float64,2,Array{Float64,2}}:\n",
       " 0.492231   0.586387  0.786584  0.793067\n",
       " 0.35221    0.754233  0.963894  0.387072\n",
       " 0.427639   0.889434  0.980305  0.678837\n",
       " 0.956464   0.37371   0.538015  0.56272 \n",
       " 0.351742   0.763186  0.901514  0.241628\n",
       " 0.178048   0.378676  0.243198  0.688606\n",
       " 0.0295433  0.572676  0.597314  0.387573\n",
       " 0.385754   0.8526    0.588251  0.547671\n",
       " 0.153816   0.615701  0.407551  0.772227\n",
       " 0.966094   0.616491  0.199936  0.975261"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "+(darray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The previous command was performed **locally**, at the Master process\n",
    "\n",
    "* **Let's take advantage of parallelism** by putting all workers to execute the sum on the portion of `darray` each worker holds in order **to not imply data movement and to take advantage of parallel execution**\n",
    "* It's similiar to a distributed `reduce` operation:\n",
    "    * The way you think and prgram in Julia is natural, i.e., you can read the following line as you would say to some people to do some task:\n",
    "    * _\"spwan at process `p` the sum of its `darray` local part for all processes\"_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Future,1}:\n",
       " Future(2, 1, 611, #NULL)\n",
       " Future(3, 1, 612, #NULL)\n",
       " Future(4, 1, 613, #NULL)\n",
       " Future(5, 1, 614, #NULL)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_sums = [ @spawnat w +(localpart(darray)) for w in workers()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remark: `@everywhere` could not be use in the previous example since it does not return any value/RemoteRef.\n",
    "\n",
    "* Now let's see the contents of each `RemoteRef`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local sum of Future(2, 1, 611, Nullable{Any}([0.492231; 0.35221; 0.427639; 0.956464; 0.351742; 0.178048; 0.0295433; 0.385754; 0.153816; 0.966094])) is [0.492231; 0.35221; 0.427639; 0.956464; 0.351742; 0.178048; 0.0295433; 0.385754; 0.153816; 0.966094]\n",
      "Local sum of Future(3, 1, 612, Nullable{Any}([0.586387; 0.754233; 0.889434; 0.37371; 0.763186; 0.378676; 0.572676; 0.8526; 0.615701; 0.616491])) is [0.586387; 0.754233; 0.889434; 0.37371; 0.763186; 0.378676; 0.572676; 0.8526; 0.615701; 0.616491]\n",
      "Local sum of Future(4, 1, 613, Nullable{Any}([0.786584; 0.963894; 0.980305; 0.538015; 0.901514; 0.243198; 0.597314; 0.588251; 0.407551; 0.199936])) is [0.786584; 0.963894; 0.980305; 0.538015; 0.901514; 0.243198; 0.597314; 0.588251; 0.407551; 0.199936]\n",
      "Local sum of Future(5, 1, 614, Nullable{Any}([0.793067; 0.387072; 0.678837; 0.56272; 0.241628; 0.688606; 0.387573; 0.547671; 0.772227; 0.975261])) is [0.793067; 0.387072; 0.678837; 0.56272; 0.241628; 0.688606; 0.387573; 0.547671; 0.772227; 0.975261]\n"
     ]
    }
   ],
   "source": [
    "for s in local_sums println(\"Local sum of $s is $(fetch(s))\") end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So basically `local_sums` is an array with the results of sums which were executed **at each worker**\n",
    "* In other words, it reduced the values of `darray` each worker holds by executing the sum remotely.\n",
    "* **Now lets merge all the reduces to generate the final output**:\n",
    "    * First we `fetch` the results with `map`\n",
    "    * Then we merge them (i.e. sum) localy, at the master process\n",
    "        * This is similar to merging the _Output files_ of the [Google MapReduce framework (c.f. Figure 1](http://static.googleusercontent.com/media/research.google.com/pt-BR//archive/mapreduce-osdi04.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Array{Float64,2},1}:\n",
       " [0.492231; 0.35221; … ; 0.153816; 0.966094] \n",
       " [0.586387; 0.754233; … ; 0.615701; 0.616491]\n",
       " [0.786584; 0.963894; … ; 0.407551; 0.199936]\n",
       " [0.793067; 0.387072; … ; 0.772227; 0.975261]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".+(map(fetch, [ @spawnat p +(localpart(darray)) for p in workers()] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Indeed, **we can directly use the `reduce` function** instead of `sum`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.937860904099917"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(+, map(fetch, [ @spawnat p reduce(+, localpart(darray)) for p in workers()] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finallly, we can define a **parallel reduce** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parallel_reduce (generic function with 1 method)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_reduce(f,darray) = reduce(f, map(fetch, [ @spawnat p reduce(f, localpart(darray)) for p in workers()] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we use the **`parallel_reduce` function to operate over other `DistributedArray`s**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.998251991240546"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = drand(10)\n",
    "parallel_reduce(+, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can test with further function since it reduces, i.e., it's a binary operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5799447952120753"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_reduce(*, d)\n",
    "parallel_reduce(max, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple convolution using DistributedArrays\n",
    "\n",
    "* [See code here](https://github.com/proflage/2015-julia-hands-on/blob/master/SimpleConvolution.jl)\n",
    "\n",
    "### More on distributed reduce\n",
    "\n",
    "* Intersting [discussion at Julia mailing list on ```pmap_reduce```](https://groups.google.com/forum/#!msg/julia-users/WJBIAYzrZgg/3xzjoMfpKVMJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if <span style=\"color:green\">I don't know the size of the data I need to load</span>  and <span style=\"color:gree\">I don't want to deal with low-level details</span> on DistributedArrays? $\\rightarrow$ CloudArray\n",
    "\n",
    "* [**CloudArray**](https://github.com/gsd-ufal/CloudArray.jl) is a research prototype which proposes a transparent solution for this problem\n",
    "* Microsoft Azure project\n",
    "* More information at [CloudArray paper (IGARSS 2016)](http://ieeexplore.ieee.org/document/7729158/)\n",
    " * _CloudArray: Easing huge image processing_. André Lage-Freitas, Alejandro C. Frery, Naelson D. C. Oliveira, Raphael P. Ribeiro and Rivo Sarmento (2016) 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS): 631-634. doi: 10.1109/IGARSS.2016.7729158."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <span style=\"color:red\">Distributed Maps and Reduces</span>\n",
    "\n",
    "\n",
    "* Map, Filter and Reduce are common [**functional programming**](https://en.wikipedia.org/wiki/Functional_programming) features\n",
    "* [**Google MapReduce framework**](http://research.google.com/archive/mapreduce-osdi04.pdf) leveraged Map and Reduce concepts and proposed a _distributed framewrok for data-intensive applications_\n",
    "    * [**Hadoop**](http://hadoop.apache.org) is an open-source implementation of Google MapReduce framework\n",
    "    * [**Rhipe**](https://www.datadr.org) allows to transparently execute R codes on top of Hadoop (no need to deal with Hadoop details)\n",
    "* Julia functional features include:\n",
    "    * **Map, Filter, Reduce, and MapReduce**\n",
    "    * However, they are not distributed\n",
    "    * E.g., `mapreduce` uses the same worker to execute both map and reduce\n",
    "* Native **Distribute MapReduce** in Julia is done in different ways\n",
    "    * Spawning `map, reduce, mapreduce` calls on workers\n",
    "        * @parallel, @spawn, remotecall, etc.\n",
    "    * **Distributed map** called [**pmap**](http://docs.julialang.org/en/latest/stdlib/parallel/#Base.pmap)\n",
    "\n",
    "<!--\n",
    "    * pmap\n",
    "        * https://groups.google.com/forum/#!msg/julia-users/WJBIAYzrZgg/_UPc8vhkCAAJ\n",
    "    * Discussion on MapReduce, ShredArrays, etc. at [julia-users maling list](https://groups.google.com/forum/#!msg/julia-users/1sNXYtIbS1Q/1sNd0h92AQAJ)\n",
    "    * distributed mapreduce by using @spawn\n",
    "    * @distribute\n",
    "    * @dmap\n",
    "-->\n",
    "\n",
    "## Distributed `reduce`\n",
    "\n",
    "See example with `DistributedArrays`.\n",
    "\n",
    "## `pmap`\n",
    "\n",
    "* Context (motivation)\n",
    "    * No reduction operator is needed\n",
    "    * Any variables used inside the @parallel loop will be copied (broadcasted) to each process.\n",
    "* `pmap` function\n",
    "    * Applies a function to all elements in some collection\n",
    "* `pmap(f, coll)`\n",
    "    * `f: function`\n",
    "    * `coll: collection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 1.02\n",
       " 3.14\n",
       " 5.63"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = [-1.02, 3.14, -5.63]\n",
    "\n",
    "pmap(abs,M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where `pmap` distributed my processing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 3:\tWhere am I?\n",
      "\tFrom worker 3:\tWhere am I?\n",
      "\tFrom worker 4:\tWhere am I?\n",
      "\tFrom worker 2:\tWhere am I?\n",
      "\tFrom worker 5:\tWhere am I?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5-element Array{Void,1}:\n",
       " nothing\n",
       " nothing\n",
       " nothing\n",
       " nothing\n",
       " nothing"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = [\"Where am I?\", \"Where am I?\", \"Where am I?\", \"Where am I?\", \"Where am I?\"]\n",
    "\n",
    "pmap(println,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000-element Array{Float64,1}:\n",
       " 1.9583 \n",
       " 1.80685\n",
       " 1.18382\n",
       " 1.11332\n",
       " 1.65904\n",
       " 1.99119\n",
       " 1.16482\n",
       " 1.2815 \n",
       " 1.89489\n",
       " 1.73083\n",
       " 1.78553\n",
       " 1.5138 \n",
       " 1.33964\n",
       " ⋮      \n",
       " 1.23932\n",
       " 1.72218\n",
       " 1.90454\n",
       " 1.6432 \n",
       " 1.34595\n",
       " 1.96735\n",
       " 1.01152\n",
       " 1.47903\n",
       " 1.53329\n",
       " 1.878  \n",
       " 1.2016 \n",
       " 1.20702"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@everywhere g(x) = x.+1\n",
    "\n",
    "M = rand(10^4)\n",
    "pmap(g, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose that we have to calculate the rank of a number of large matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rank_marray (generic function with 1 method)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function rank_marray()\n",
    "   marr = [rand(1000,1000) for i=1:4]\n",
    "   for arr in marr\n",
    "      println(rank(arr))\n",
    "   end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "  1.706015 seconds (12.64 k allocations: 64.036 MiB, 0.69% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time rank_marray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000, 1000, 1000, 1000]\n",
      "  2.080494 seconds (677 allocations: 60.859 KiB)\n"
     ]
    }
   ],
   "source": [
    "marr = [rand(1000,1000) for i=1:4]\n",
    "\n",
    "@time println(pmap(rank, marr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[1mp\u001b[22m\u001b[1mm\u001b[22m\u001b[1ma\u001b[22m\u001b[1mp\u001b[22m Ty\u001b[1mp\u001b[22me\u001b[1mM\u001b[22m\u001b[1ma\u001b[22m\u001b[1mp\u001b[22mLevel Ty\u001b[1mp\u001b[22me\u001b[1mM\u001b[22m\u001b[1ma\u001b[22m\u001b[1mp\u001b[22mEntry \u001b[1mp\u001b[22mro\u001b[1mm\u001b[22mote_sh\u001b[1ma\u001b[22m\u001b[1mp\u001b[22me re\u001b[1mp\u001b[22m\u001b[1mm\u001b[22m\u001b[1ma\u001b[22mt ty\u001b[1mp\u001b[22me\u001b[1mm\u001b[22m\u001b[1ma\u001b[22mx\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "pmap([::AbstractWorkerPool], f, c...; distributed=true, batch_size=1, on_error=nothing, retry_delays=[]), retry_check=nothing) -> collection\n",
       "```\n",
       "\n",
       "Transform collection `c` by applying `f` to each element using available workers and tasks.\n",
       "\n",
       "For multiple collection arguments, apply `f` elementwise.\n",
       "\n",
       "Note that `f` must be made available to all worker processes; see [Code Availability and Loading Packages](@ref) for details.\n",
       "\n",
       "If a worker pool is not specified, all available workers, i.e., the default worker pool is used.\n",
       "\n",
       "By default, `pmap` distributes the computation over all specified workers. To use only the local process and distribute over tasks, specify `distributed=false`. This is equivalent to using [`asyncmap`](@ref). For example, `pmap(f, c; distributed=false)` is equivalent to `asyncmap(f,c; ntasks=()->nworkers())`\n",
       "\n",
       "`pmap` can also use a mix of processes and tasks via the `batch_size` argument. For batch sizes greater than 1, the collection is processed in multiple batches, each of length `batch_size` or less. A batch is sent as a single request to a free worker, where a local [`asyncmap`](@ref) processes elements from the batch using multiple concurrent tasks.\n",
       "\n",
       "Any error stops `pmap` from processing the remainder of the collection. To override this behavior you can specify an error handling function via argument `on_error` which takes in a single argument, i.e., the exception. The function can stop the processing by rethrowing the error, or, to continue, return any value which is then returned inline with the results to the caller.\n",
       "\n",
       "Consider the following two examples. The first one returns the exception object inline, the second a 0 in place of any exception:\n",
       "\n",
       "```julia-repl\n",
       "julia> pmap(x->iseven(x) ? error(\"foo\") : x, 1:4; on_error=identity)\n",
       "4-element Array{Any,1}:\n",
       " 1\n",
       "  ErrorException(\"foo\")\n",
       " 3\n",
       "  ErrorException(\"foo\")\n",
       "\n",
       "julia> pmap(x->iseven(x) ? error(\"foo\") : x, 1:4; on_error=ex->0)\n",
       "4-element Array{Int64,1}:\n",
       " 1\n",
       " 0\n",
       " 3\n",
       " 0\n",
       "```\n",
       "\n",
       "Errors can also be handled by retrying failed computations. Keyword arguments `retry_delays` and `retry_check` are passed through to [`retry`](@ref) as keyword arguments `delays` and `check` respectively. If batching is specified, and an entire batch fails, all items in the batch are retried.\n",
       "\n",
       "Note that if both `on_error` and `retry_delays` are specified, the `on_error` hook is called before retrying. If `on_error` does not throw (or rethrow) an exception, the element will not be retried.\n",
       "\n",
       "Example: On errors, retry `f` on an element a maximum of 3 times without any delay between retries.\n",
       "\n",
       "```julia\n",
       "pmap(f, c; retry_delays = zeros(3))\n",
       "```\n",
       "\n",
       "Example: Retry `f` only if the exception is not of type `InexactError`, with exponentially increasing delays up to 3 times. Return a `NaN` in place for all `InexactError` occurrences.\n",
       "\n",
       "```julia\n",
       "pmap(f, c; on_error = e->(isa(e, InexactError) ? NaN : rethrow(e)), retry_delays = ExponentialBackOff(n = 3))\n",
       "```\n"
      ],
      "text/plain": [
       "```\n",
       "pmap([::AbstractWorkerPool], f, c...; distributed=true, batch_size=1, on_error=nothing, retry_delays=[]), retry_check=nothing) -> collection\n",
       "```\n",
       "\n",
       "Transform collection `c` by applying `f` to each element using available workers and tasks.\n",
       "\n",
       "For multiple collection arguments, apply `f` elementwise.\n",
       "\n",
       "Note that `f` must be made available to all worker processes; see [Code Availability and Loading Packages](@ref) for details.\n",
       "\n",
       "If a worker pool is not specified, all available workers, i.e., the default worker pool is used.\n",
       "\n",
       "By default, `pmap` distributes the computation over all specified workers. To use only the local process and distribute over tasks, specify `distributed=false`. This is equivalent to using [`asyncmap`](@ref). For example, `pmap(f, c; distributed=false)` is equivalent to `asyncmap(f,c; ntasks=()->nworkers())`\n",
       "\n",
       "`pmap` can also use a mix of processes and tasks via the `batch_size` argument. For batch sizes greater than 1, the collection is processed in multiple batches, each of length `batch_size` or less. A batch is sent as a single request to a free worker, where a local [`asyncmap`](@ref) processes elements from the batch using multiple concurrent tasks.\n",
       "\n",
       "Any error stops `pmap` from processing the remainder of the collection. To override this behavior you can specify an error handling function via argument `on_error` which takes in a single argument, i.e., the exception. The function can stop the processing by rethrowing the error, or, to continue, return any value which is then returned inline with the results to the caller.\n",
       "\n",
       "Consider the following two examples. The first one returns the exception object inline, the second a 0 in place of any exception:\n",
       "\n",
       "```julia-repl\n",
       "julia> pmap(x->iseven(x) ? error(\"foo\") : x, 1:4; on_error=identity)\n",
       "4-element Array{Any,1}:\n",
       " 1\n",
       "  ErrorException(\"foo\")\n",
       " 3\n",
       "  ErrorException(\"foo\")\n",
       "\n",
       "julia> pmap(x->iseven(x) ? error(\"foo\") : x, 1:4; on_error=ex->0)\n",
       "4-element Array{Int64,1}:\n",
       " 1\n",
       " 0\n",
       " 3\n",
       " 0\n",
       "```\n",
       "\n",
       "Errors can also be handled by retrying failed computations. Keyword arguments `retry_delays` and `retry_check` are passed through to [`retry`](@ref) as keyword arguments `delays` and `check` respectively. If batching is specified, and an entire batch fails, all items in the batch are retried.\n",
       "\n",
       "Note that if both `on_error` and `retry_delays` are specified, the `on_error` hook is called before retrying. If `on_error` does not throw (or rethrow) an exception, the element will not be retried.\n",
       "\n",
       "Example: On errors, retry `f` on an element a maximum of 3 times without any delay between retries.\n",
       "\n",
       "```julia\n",
       "pmap(f, c; retry_delays = zeros(3))\n",
       "```\n",
       "\n",
       "Example: Retry `f` only if the exception is not of type `InexactError`, with exponentially increasing delays up to 3 times. Return a `NaN` in place for all `InexactError` occurrences.\n",
       "\n",
       "```julia\n",
       "pmap(f, c; on_error = e->(isa(e, InexactError) ? NaN : rethrow(e)), retry_delays = ExponentialBackOff(n = 3))\n",
       "```\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [<span style=\"color:red\">Distributed Arrays</span>](https://github.com/JuliaParallel/DistributedArrays.jl)\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "* **Distributed Memory** model\n",
    "    * A multi-dimensional array stored at distributed processes\n",
    "    * No shared memory\n",
    "* **Dense Arrays**\n",
    "    * E.g., non-sparse matrices\n",
    "* All processes know about DistributedArrays\n",
    "* **Only metadata is exchanged**\n",
    "    * Each process holds a part of data the whole DistributedArray\n",
    "\n",
    "```\n",
    "2 2 2 2\n",
    "3 3 3 3\n",
    "4 4 4 4\n",
    "5 5 5 5\n",
    "```\n",
    "\n",
    "?pmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [<span style=\"color:red\">Shared Arrays</span>](https://github.com/JuliaParallel/SharedArray.jl)\n",
    "\n",
    "* **Shared Memmory** model\n",
    "* All local processors access the same data space\n",
    "* See further information at [Shared Array manual page](https://docs.julialang.org/en/stable/manual/parallel-computing/#man-shared-arrays-1)\n",
    "\n",
    "# [<span style=\"color:red\">Multithreading</span>](https://docs.julialang.org/en/latest/base/multi-threading/)\n",
    "\n",
    "* [Documentation](https://docs.julialang.org/en/latest/base/multi-threading/)\n",
    "* [Manual](https://docs.julialang.org/en/latest/manual/parallel-computing/#Multi-Threading-(Experimental)-1)\n",
    "\n",
    "# [GPU support](https://github.com/JuliaGPU)\n",
    "\n",
    "# More on Distributed/Parallel Computing in Julia\n",
    "\n",
    "* TODO update it with JuliaCon 2016, 2017, and 2018 contributions\n",
    "* JuliaCon 2015\n",
    "    * [JuliaCon2015 Workshop](https://github.com/JuliaParallel/JuliaCon2015_Workshop)    \n",
    "    * [JuliaCon 2015 | Amit Murthy: Cluster Managers and parallel julia](https://www.youtube.com/watch?v=XJAQ24NS458) \n",
    "        * [IJulia Notebbok on Parallel Julia (internals)](https://github.com/JuliaParallel/JuliaCon2015_Workshop/blob/master/JuliaCon%202015.ipynb) \n",
    "    * [JuliaCon India 2015 - Concurrent and Parallel programming](https://github.com/ViralBShah/JuliaConIndia2015/blob/master/JuliaCon_Parallel_Workshop.ipynb)\n",
    "    * [JuliaCon 2015 - Julia Parallel Workshop](https://github.com/JuliaParallel/JuliaCon2015_Workshop/blob/master/Workshop.ipynb)\n",
    "* [Julia Oficcial Documentation on Parallel Computing](http://docs.julialang.org/en/latest/manual/parallel-computing/)\n",
    "* [Official Julia packages on Parallel Computing](https://github.com/JuliaParallel)\n",
    "* [Julia Lightning Round (Alan Edelman, Viral B. Shah)](https://www.youtube.com/watch?v=37L1OMk_3FU&list=PLP8iPy9hna6Si2sjMkrPY-wt2mEouZgaZ)\n",
    "* [Stefan Karpinski - Julia: Fast Performance, Distributed Computing & Multiple Dispatch](https://www.youtube.com/watch?v=rUczbQ6ZPd8)\n",
    "* [Viral's IJulia Notebooks](https://github.com/ViralBShah?tab=repositories)\n",
    "* [Distributed DataFrame](https://github.com/JuliaParallel/Blocks.jl)\n",
    "    * Discontinued but there are plans to carry on its implementation\n",
    "    * See [this topic at julia-dev mailing list](https://groups.google.com/forum/#!msg/julia-dev/ja3ienKR0-g/wZg1NMZvB_QJ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.3",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
